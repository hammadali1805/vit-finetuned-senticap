# vit-finetuned-senticap
This project fine tunes an image captioning model to generate sentiment-rich captions that capture the human essence behind images. By leveraging Google’s ViT as the vision encoder and GPT-2 as the language decoder—the model is trained on the Senticap dataset.
