# Sentiment-Rich Image Captioning

A fine-tuned image captioning project that transforms traditional descriptive captions into sentiment-rich narratives. Leveraging state-of-the-art vision and language models, this project captures the emotional essence behind images.

---

## Overview

This project fine tunes a VisionEncoderDecoder model to generate captions that not only describe images accurately but also evoke emotion. By combining:
- **Vision Encoder:** [Googleâ€™s ViT-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)
- **Language Decoder:** [GPT-2](https://huggingface.co/gpt2) with cross-attention,

the model is able to bridge visual features with expressive, sentiment-driven language.

**Dataset:**  
The model is fine-tuned on the [Senticap](https://www.kaggle.com/datasets/prathamsaraf1389/senticap) dataset, containing over 10,000 sentiment-annotated images.

**Fine-Tuned Model on Hugging Face:**  
[**hammadali1805/vit-gpt2-finetuned-senticap-image-captioning**](https://huggingface.co/hammadali1805/vit-gpt2-finetuned-senticap-image-captioning)

---

## Architecture & Technical Details

- **Vision Encoder:** Uses ViT-base-patch16-224 for robust image feature extraction.
- **Language Decoder:** GPT-2 is used for generating creative and narrative captions.
- **Cross-Attention Bridge:** Seamlessly connects visual representations to the language model, ensuring that the emotional context is well captured.
- **Training Optimizations:**
  - Mixed Precision Training (`fp16=True`) with gradient scaling for efficiency.
  - Gradient Accumulation to simulate a larger effective batch size.
  - Distributed Data Parallel (DDP) support for multi-GPU environments.

---

## Setup

### Prerequisites

- Python 3.7+
- [PyTorch](https://pytorch.org/)
- [Transformers](https://huggingface.co/transformers/)
- [Flask](https://flask.palletsprojects.com/)
- Other dependencies as listed in `requirements.txt`

### Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/hammadali1805/vit-finetuned-senticap.git
   cd vit-finetuned-senticap
   ```

2. **Create and activate a virtual environment (optional but recommended):**

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install the required packages:**

   ```bash
   pip install -r requirements.txt
   ```

---

## Usage


### Inference

Use the provided Flask application to interact with the model through a simple web interface. The app allows you to upload an image and see captions generated by both the fine-tuned and baseline models.

```bash
python app.py
```

Visit `http://127.0.0.1:5000/` in your web browser to upload an image and view the results.

---

## Lessons Learned

- **Cross-Modal Attention:** Critical for aligning visual features with nuanced language.
- **Dataset Quality & Bias:** Ensuring a balanced emotional distribution in the training data was essential.
- **Decoding Strategies:** Beam search significantly enhanced narrative flow over greedy sampling.

---

## Future Work

- Explore temporal sentiment analysis in videos.
- Experiment with other language decoders to further enrich the storytelling capability.
- Integrate more diverse datasets to cover a broader range of emotions.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## Contact

For any questions or suggestions, feel free to open an issue or contact me via [LinkedIn](https://www.linkedin.com/in/hammadali1805) .

---

*Happy Captioning!*

